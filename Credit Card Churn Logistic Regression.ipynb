{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b631464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7abbf9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in dataset\n",
    "df = pd.read_csv(\"/Users/cartermain/Downloads/BankChurners.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d18471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CLIENTNUM     Attrition_Flag  Customer_Age Gender  Dependent_count  \\\n",
      "0  768805383  Existing Customer            45      M                3   \n",
      "1  818770008  Existing Customer            49      F                5   \n",
      "2  713982108  Existing Customer            51      M                3   \n",
      "\n",
      "  Education_Level Marital_Status Income_Category Card_Category  \\\n",
      "0     High School        Married     $60K - $80K          Blue   \n",
      "1        Graduate         Single  Less than $40K          Blue   \n",
      "2        Graduate        Married    $80K - $120K          Blue   \n",
      "\n",
      "   Months_on_book  ...  Months_Inactive_12_mon  Contacts_Count_12_mon  \\\n",
      "0              39  ...                       1                      3   \n",
      "1              44  ...                       1                      2   \n",
      "2              36  ...                       1                      0   \n",
      "\n",
      "   Credit_Limit  Total_Revolving_Bal  Avg_Open_To_Buy  Total_Amt_Chng_Q4_Q1  \\\n",
      "0       12691.0                  777          11914.0                 1.335   \n",
      "1        8256.0                  864           7392.0                 1.541   \n",
      "2        3418.0                    0           3418.0                 2.594   \n",
      "\n",
      "   Total_Trans_Amt  Total_Trans_Ct  Total_Ct_Chng_Q4_Q1  Avg_Utilization_Ratio  \n",
      "0             1144              42                1.625                  0.061  \n",
      "1             1291              33                3.714                  0.105  \n",
      "2             1887              20                2.333                  0.000  \n",
      "\n",
      "[3 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# getting familiar with dataframe\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe29fc",
   "metadata": {},
   "source": [
    "We're going to use a logistic regression model to predict churn probability for cardholders but first we need to clean up our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551c04f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIENTNUM                   0\n",
      "Attrition_Flag              0\n",
      "Customer_Age                0\n",
      "Gender                      0\n",
      "Dependent_count             0\n",
      "Education_Level             0\n",
      "Marital_Status              0\n",
      "Income_Category             0\n",
      "Card_Category               0\n",
      "Months_on_book              0\n",
      "Total_Relationship_Count    0\n",
      "Months_Inactive_12_mon      0\n",
      "Contacts_Count_12_mon       0\n",
      "Credit_Limit                0\n",
      "Total_Revolving_Bal         0\n",
      "Avg_Open_To_Buy             0\n",
      "Total_Amt_Chng_Q4_Q1        0\n",
      "Total_Trans_Amt             0\n",
      "Total_Trans_Ct              0\n",
      "Total_Ct_Chng_Q4_Q1         0\n",
      "Avg_Utilization_Ratio       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# counting null values within dataframe\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf18dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using interquartile range to turn outliers of columns with int and float dtypes into null values\n",
    "for feature in df.columns:\n",
    "    if df[feature].dtype == \"object\":\n",
    "        continue\n",
    "    else:\n",
    "        for x in df[feature].sort_values(ascending = False)[0:100]:\n",
    "            q75 = df[feature].describe()[\"75%\"]\n",
    "            q25 = df[feature].describe()[\"25%\"]\n",
    "            intr_qr = q75 - q25\n",
    "            max = q75 + (1.5 * intr_qr)\n",
    "            min = q25 - (1.5 * intr_qr)\n",
    "            if x > max or x < min:\n",
    "                df.loc[df[feature] == x, feature] = np.nan\n",
    "        for x in df[feature][df[feature].notnull() == True].sort_values(ascending = False)[-100:]:\n",
    "            if x > max or x < min:\n",
    "                df.loc[df[feature] == x, feature] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587127f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIENTNUM                     0\n",
      "Attrition_Flag                0\n",
      "Customer_Age                  2\n",
      "Gender                        0\n",
      "Dependent_count               0\n",
      "Education_Level               0\n",
      "Marital_Status                0\n",
      "Income_Category               0\n",
      "Card_Category                 0\n",
      "Months_on_book              223\n",
      "Total_Relationship_Count      0\n",
      "Months_Inactive_12_mon      153\n",
      "Contacts_Count_12_mon       629\n",
      "Credit_Limit                508\n",
      "Total_Revolving_Bal           0\n",
      "Avg_Open_To_Buy             100\n",
      "Total_Amt_Chng_Q4_Q1        148\n",
      "Total_Trans_Amt             101\n",
      "Total_Trans_Ct                2\n",
      "Total_Ct_Chng_Q4_Q1         203\n",
      "Avg_Utilization_Ratio         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# counting null values after dropping outliers \n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f079567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping null values our for loop identified as outliers\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508b8ff",
   "metadata": {},
   "source": [
    "We're going to need to turn all columns into an int or float dtype to work within our logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083c4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using label encoder to turn classification columns with \"object\" dtype into int \n",
    "# and collecting column names to drop when we create feature set\n",
    "le = preprocessing.LabelEncoder()\n",
    "object_columns = []\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == \"object\":\n",
    "        df[column + \"_class\"] = le.fit_transform(df[column])\n",
    "        object_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7d1bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting features and dropping columns we collected at last step with \"object\" dtype \n",
    "# as well as our target and client number\n",
    "features = df.drop(columns = object_columns)\n",
    "features = features.drop(columns = \"Attrition_Flag_class\")\n",
    "features = features.drop(columns = \"CLIENTNUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a82b4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29929769, -0.28936318,  0.00837525, ..., -0.62768276,\n",
       "        -1.28941323, -0.1971452 ],\n",
       "       [-0.5594046 ,  2.04872872, -0.66682905, ...,  2.07358903,\n",
       "        -1.96567778, -0.1971452 ],\n",
       "       [ 2.43182492, -1.06872715,  2.43911075, ..., -0.62768276,\n",
       "        -1.28941323, -0.1971452 ],\n",
       "       ...,\n",
       "       [-0.29929769, -1.06872715,  0.00837525, ..., -0.62768276,\n",
       "         0.73938041, -0.1971452 ],\n",
       "       [-2.12004609, -0.28936318,  0.00837525, ...,  2.07358903,\n",
       "        -1.28941323, -0.1971452 ],\n",
       "       [-0.42935115, -0.28936318, -1.47707421, ..., -0.62768276,\n",
       "         0.73938041,  5.24458467]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling feature set for use in logistic regression model since features are on different sclaes\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d429f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test splitting with random state of 42 so we can accurately assess accuracy improvements later on\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, df[\"Attrition_Flag_class\"], train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7fa44b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.884046381447421\n"
     ]
    }
   ],
   "source": [
    "# fitting and scoring the first iteration of our logistic regression model\n",
    "lr = LogisticRegression(max_iter = 5000)\n",
    "lr.fit(x_train, y_train)\n",
    "print(lr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc74266",
   "metadata": {},
   "source": [
    "A strong start. Let's run feature selection via RFE to see if we can match or improve our model's accuracy with fewer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ece3a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8932427029188325 17\n"
     ]
    }
   ],
   "source": [
    "# running RFE through a for loop to determine which combination of features yield the highest accuracy\n",
    "max_score = 0\n",
    "best_x = 0\n",
    "for x in range(1,len(features.columns) + 1):\n",
    "    rfe = RFE(estimator = lr, n_features_to_select = x)\n",
    "    rfe.fit(x_train, y_train)\n",
    "    score = rfe.score(x_test, y_test)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        best_x = x\n",
    "print(max_score, best_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aba7a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True False  True False  True  True  True\n",
      "  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# training RFE outside of loop with best number of features so we can find the features it recommends keeping\n",
    "rfe = RFE(estimator = lr, n_features_to_select = best_x)\n",
    "rfe.fit(x_train, y_train)\n",
    "print(rfe.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c73cbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting the feature names we will end up keeping in our revised feature set\n",
    "kept_features = []\n",
    "x = 0\n",
    "for kept in rfe.support_:\n",
    "    if kept == True:\n",
    "        kept_features.append(features.columns[x])\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a04abf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new feature set with the 17 features RFE determined generated the highest accuracy\n",
    "features_2 = df[kept_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "532faba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29929769, -0.28936318,  0.00837525, ..., -0.62768276,\n",
       "        -1.28941323, -0.1971452 ],\n",
       "       [-0.5594046 ,  2.04872872, -0.66682905, ...,  2.07358903,\n",
       "        -1.96567778, -0.1971452 ],\n",
       "       [ 2.43182492, -1.06872715,  2.43911075, ..., -0.62768276,\n",
       "        -1.28941323, -0.1971452 ],\n",
       "       ...,\n",
       "       [-0.29929769, -1.06872715,  0.00837525, ..., -0.62768276,\n",
       "         0.73938041, -0.1971452 ],\n",
       "       [-2.12004609, -0.28936318,  0.00837525, ...,  2.07358903,\n",
       "        -1.28941323, -0.1971452 ],\n",
       "       [-0.42935115, -0.28936318, -1.47707421, ..., -0.62768276,\n",
       "         0.73938041,  5.24458467]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling our revised feature set for use in logistic regression\n",
    "scaler_2 = StandardScaler()\n",
    "scaler_2.fit_transform(features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dbe5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test splitting revised feature set with same random state split\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(features_2, df[\"Attrition_Flag_class\"], train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f21935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8932427029188325\n"
     ]
    }
   ],
   "source": [
    "# retraining our logistic regression model with our split and scaled new feature set\n",
    "lr.fit(x_train_2, y_train_2)\n",
    "print(lr.score(x_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f4225",
   "metadata": {},
   "source": [
    "Nice, we were able to improve the accuracy of our model and drop 2 features. Now let's use random search to check if there are any better-performing hyperparameters than the logistic regression defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eeff943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up dictionary of hyperparameters (solver and C values) to test via randomized search\n",
    "params = {\"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"], \"C\": range(0, 30, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67bb6025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "35 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 434, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only solvers in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], got newton-cholesky.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1589, in fit\n",
      "    fold_coefs_ = Parallel(\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 811, in _logistic_regression_path\n",
      "    args=(X, target, 1.0 / C, sample_weight),\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1528, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py\", line 1186, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn/svm/_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1589, in fit\n",
      "    fold_coefs_ = Parallel(\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 858, in _logistic_regression_path\n",
      "    alpha = 1.0 / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.86752356 0.89717224        nan 0.86752356 0.86546701        nan\n",
      " 0.89734362 0.86752356 0.90608398 0.86546701        nan        nan\n",
      " 0.90779777 0.90608398 0.89648672        nan 0.86546701        nan\n",
      " 0.89820051        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=LogisticRegression(max_iter=5000), n_iter=20,\n",
       "                   param_distributions={'C': range(0, 30, 5),\n",
       "                                        'solver': ['lbfgs', 'liblinear',\n",
       "                                                   'newton-cg',\n",
       "                                                   'newton-cholesky', 'sag',\n",
       "                                                   'saga']})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running our random search to indentify any hyperparameter combinations that outperform defaults\n",
    "# some of these won't converge but \n",
    "randomsearch = RandomizedSearchCV(lr, params, n_iter = 20)\n",
    "randomsearch.fit(x_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80963358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8972411035585766\n",
      "{'solver': 'newton-cg', 'C': 15}\n"
     ]
    }
   ],
   "source": [
    "# finding best score from our random search test and corresponding hyperparameters\n",
    "print(randomsearch.score(x_test_2, y_test_2))\n",
    "print(randomsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b9c36",
   "metadata": {},
   "source": [
    "We were able to find a slightly better score than our default values with newton_cg as the solver and a C of 10. Let's try a few different C values via gridsearchCV and compare the default solver with newton-cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4715a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a gridsearchCV to test a few targeted hyperparams based on results of randomsearch\n",
    "gridsearch_params = {\"solver\": [\"newton-cg\", \"lbfgs\"], \"C\": range(5, 20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ceaf9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(max_iter=5000),\n",
       "             param_grid={'C': range(5, 20), 'solver': ['newton-cg', 'lbfgs']})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running gridsearchCV with the different C values identified in the previous step\n",
    "gridsearch = GridSearchCV(lr, gridsearch_params)\n",
    "gridsearch.fit(x_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17c5876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8972411035585766\n",
      "{'C': 9, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "# finding best score and corresponding params\n",
    "print(gridsearch.score(x_test_2, y_test_2))\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ccb0a0",
   "metadata": {},
   "source": [
    "Let's update our live model with the hyperparams that resulted in the most accurate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "461b89df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8972411035585766\n"
     ]
    }
   ],
   "source": [
    "# updating our logistic regression model based on best found C value and solver\n",
    "lr = LogisticRegression(solver = gridsearch.best_params_[\"solver\"], C = gridsearch.best_params_[\"C\"], max_iter = 5000)\n",
    "lr.fit(x_train_2, y_train_2)\n",
    "print(lr.score(x_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc0666",
   "metadata": {},
   "source": [
    "As a final step, let's test different train sizes for our split to find the train size that results in the highest accuracy score for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79e09816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:415: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9054146675805346 0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "# now, let's find the best train test split that avoids overfitting while using same random state to ensure a 1:1 comparison\n",
    "best_score = 0\n",
    "best_split = 0\n",
    "for x in np.linspace(0,1,21):\n",
    "    if x == 0 or x == 1:\n",
    "        continue\n",
    "    else:\n",
    "        x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(features_2, df[\"Attrition_Flag_class\"], train_size = x, random_state = 42)\n",
    "        lr.fit(x_train_3, y_train_3)\n",
    "        score = lr.score(x_test_3, y_test_3)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_split = x\n",
    "print(best_score, best_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c88c26",
   "metadata": {},
   "source": [
    "Picked up another incremental accuracy improvement. Let's update our train test split to train our final model using the split that resulted in the most accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b916137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting our dataset one final time with the best found train size\n",
    "best_x_train, best_x_test, best_y_train, best_y_test = train_test_split(features_2, df[\"Attrition_Flag_class\"], train_size = best_split, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b69aa2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9054146675805346\n"
     ]
    }
   ],
   "source": [
    "# training and scoring our model with the newly-created train test split\n",
    "lr.fit(best_x_train, best_y_train)\n",
    "print(lr.score(best_x_test, best_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26773c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              coef\n",
      "Customer_Age              0.016091\n",
      "Dependent_count          -0.122481\n",
      "Months_on_book            0.006188\n",
      "Total_Relationship_Count  0.460549\n",
      "Months_Inactive_12_mon   -0.616461\n",
      "Contacts_Count_12_mon    -0.299435\n",
      "Total_Revolving_Bal       0.001018\n",
      "Total_Amt_Chng_Q4_Q1      0.426151\n",
      "Total_Trans_Amt          -0.000466\n",
      "Total_Trans_Ct            0.125815\n",
      "Total_Ct_Chng_Q4_Q1       3.108323\n",
      "Avg_Utilization_Ratio    -0.156064\n",
      "Gender_class              0.644884\n",
      "Education_Level_class    -0.023345\n",
      "Marital_Status_class     -0.313056\n",
      "Income_Category_class    -0.052724\n",
      "Card_Category_class       0.192352\n"
     ]
    }
   ],
   "source": [
    "# finding coefficient of each feature in final version of our Logistic Regresison model\n",
    "coef_df = pd.DataFrame(index = features_2.columns, columns = [\"coef\"], data = lr.coef_[0])\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82ce14",
   "metadata": {},
   "source": [
    "We were able to get our model's accuracy over 90% with a few optimizations. Now, let's run a few sample customers through and find the probability that they will churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "448870a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in sample dataset\n",
    "sample_df = pd.read_csv(\"/Users/cartermain/Downloads/Churn Sample Set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edd36d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Customer_Age  Dependent_count  Months_on_book  Total_Relationship_Count  \\\n",
      "0            48                2              30                         4   \n",
      "1            53                0              46                         2   \n",
      "2            28                0              20                         0   \n",
      "\n",
      "   Months_Inactive_12_mon  Contacts_Count_12_mon  Total_Revolving_Bal  \\\n",
      "0                       0                      1                  750   \n",
      "1                       0                      3                  617   \n",
      "2                       2                      5                   86   \n",
      "\n",
      "   Total_Amt_Chng_Q4_Q1  Total_Trans_Amt  Total_Trans_Ct  Total_Ct_Chng_Q4_Q1  \\\n",
      "0                  0.78             2870              21                 0.33   \n",
      "1                  0.81             8223              62                 0.76   \n",
      "2                  0.25              609              30                 0.54   \n",
      "\n",
      "   Avg_Utilization_Ratio  Gender_class  Education_Level_class  \\\n",
      "0                   0.11             1                      0   \n",
      "1                   0.33             0                      4   \n",
      "2                   0.07             0                      1   \n",
      "\n",
      "   Marital_Status_class  Income_Category_class  Card_Category_class  \n",
      "0                     1                      3                    0  \n",
      "1                     0                      3                    2  \n",
      "2                     3                      5                    3  \n"
     ]
    }
   ],
   "source": [
    "# getting familiar with dataset\n",
    "print(sample_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79df1322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69944776 0.30055224]\n",
      " [0.13426406 0.86573594]\n",
      " [0.98426819 0.01573181]\n",
      " [0.7848634  0.2151366 ]\n",
      " [0.00147378 0.99852622]]\n"
     ]
    }
   ],
   "source": [
    "print(lr.predict_proba(sample_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5dc2b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding model-predicted probability of churn to our sample dataframe as a new column\n",
    "sample_df[\"churn_proba\"] = lr.predict_proba(sample_df)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6ace3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding model-predicted churn class to our sample dataframe as new column based on churn probability\n",
    "sample_df[\"churn_class\"] = sample_df[\"churn_proba\"].apply(lambda row: 0 if row >= 0.50 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4551c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Customer_Age  Dependent_count  Months_on_book  Total_Relationship_Count  \\\n",
      "0            48                2              30                         4   \n",
      "1            53                0              46                         2   \n",
      "2            28                0              20                         0   \n",
      "3            36                4              12                         3   \n",
      "4            68                2              87                         5   \n",
      "\n",
      "   Months_Inactive_12_mon  Contacts_Count_12_mon  Total_Revolving_Bal  \\\n",
      "0                       0                      1                  750   \n",
      "1                       0                      3                  617   \n",
      "2                       2                      5                   86   \n",
      "3                       1                      0                    0   \n",
      "4                       0                      2                    0   \n",
      "\n",
      "   Total_Amt_Chng_Q4_Q1  Total_Trans_Amt  Total_Trans_Ct  Total_Ct_Chng_Q4_Q1  \\\n",
      "0                  0.78             2870              21                 0.33   \n",
      "1                  0.81             8223              62                 0.76   \n",
      "2                  0.25              609              30                 0.54   \n",
      "3                  1.01            12100              49                 0.98   \n",
      "4                  1.23             1733              55                 1.43   \n",
      "\n",
      "   Avg_Utilization_Ratio  Gender_class  Education_Level_class  \\\n",
      "0                   0.11             1                      0   \n",
      "1                   0.33             0                      4   \n",
      "2                   0.07             0                      1   \n",
      "3                   0.56             1                      0   \n",
      "4                   0.76             0                      0   \n",
      "\n",
      "   Marital_Status_class  Income_Category_class  Card_Category_class  \\\n",
      "0                     1                      3                    0   \n",
      "1                     0                      3                    2   \n",
      "2                     3                      5                    3   \n",
      "3                     2                      1                    3   \n",
      "4                     3                      2                    1   \n",
      "\n",
      "   churn_proba  churn_class  \n",
      "0     0.699448            0  \n",
      "1     0.134264            1  \n",
      "2     0.984268            0  \n",
      "3     0.784863            0  \n",
      "4     0.001474            1  \n"
     ]
    }
   ],
   "source": [
    "# printing head to view results\n",
    "print(sample_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
